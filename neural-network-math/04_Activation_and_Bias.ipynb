{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: 激活函数与偏置(Activation & Bias) - 决策的开关\n",
    "\n",
    "我们已经非常接近完成一个神经网络层的完整前向传播计算了！只剩下最后两个小但至关重要的概念：偏置（Bias）和激活函数（Activation Function）。\n",
    "\n",
    "**目标:** 理解偏置和激活函数的作用，并将它们与我们之前学到的矩阵乘法结合起来，完成一次完整的层计算。\n",
    "\n",
    "**核心直觉:**\n",
    "- **加权和 (Weighted Sum):** 我们在上一课计算出的结果 `recipes @ fruits`。\n",
    "- **偏置 (Bias):** 就像一个调光器的“基础亮度”。它是一个加到加权和上的数值，决定了要多强的信号才能让这个神经元“亮”起来。如果偏置很大，神经元就更容易被激活。\n",
    "- **激活函数 (Activation Function):** 就是那个“调光器”本身。它接收“加权和 + 偏置”的总信号，然后决定神经元最终的输出亮度。这个函数是非线性的，这是神经网络能够学习复杂模式的关键。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 偏置 (Bias) - 调整激活阈值\n",
    "\n",
    "偏置是一个向量，它的长度与该层神经元的数量相同。它被简单地加到加权和的结果上。\n",
    "\n",
    "`z = (权重矩阵 @ 输入向量) + 偏置向量`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复用上一课的例子\n",
    "fruits = np.array([2, 3, 1])\n",
    "recipes = np.array([\n",
    "    [0.5, 0.8, 0.2],  # 热带果汁\n",
    "    [0.1, 0.0, 1.2]   # 柑橘果汁\n",
    "])\n",
    "\n",
    "# 假设我们的两个“果汁”神经元有不同的偏置\n",
    "# 热带果汁神经元偏置为 0 (不偏不倚)\n",
    "# 柑橘果汁神经元偏置为 -0.5 (更难被激活)\n",
    "biases = np.array([0, -0.5])\n",
    "\n",
    "# 计算加权和\n",
    "weighted_sum = recipes @ fruits\n",
    "\n",
    "# 加上偏置\n",
    "z = weighted_sum + biases\n",
    "\n",
    "print(f\"加权和: {weighted_sum}\")\n",
    "print(f\"偏置: {biases}\")\n",
    "print(f\"加权和 + 偏置: {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 激活函数 - 引入非线性\n",
    "\n",
    "如果神经网络只是一系列的矩阵乘法和加法，那么它本质上只是一个线性函数，无法学习像图像识别这样复杂的任务。激活函数的作用就是引入“非线性”。\n",
    "\n",
    "我们将介绍两个经典的激活函数：Sigmoid 和 ReLU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 函数\n",
    "\n",
    "Sigmoid 函数会将任何输入的实数“挤压”到 0 和 1 之间。它看起来像一个平滑的 'S' 形曲线。在早期的神经网络中非常流行。\n",
    "\n",
    "数学公式: `σ(x) = 1 / (1 + e^(-x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit) 函数\n",
    "\n",
    "ReLU 是目前最常用的激活函数，它非常简单高效。如果输入大于0，它就直接输出该值；如果输入小于或等于0，它就输出0。\n",
    "\n",
    "数学公式: `ReLU(x) = max(0, x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一系列输入值\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# 计算对应的激活值\n",
    "sigmoid_y = sigmoid(x)\n",
    "relu_y = relu(x)\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Sigmoid 子图\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid_y)\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output (activation)')\n",
    "plt.grid(True)\n",
    "\n",
    "# ReLU 子图\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, relu_y)\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output (activation)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 整合：一个完整层的计算\n",
    "\n",
    "现在，我们将所有部分组合起来！\n",
    "\n",
    "`activation = activation_function( (weights @ inputs) + biases )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们的输入\n",
    "inputs = fruits\n",
    "\n",
    "# 我们的参数\n",
    "weights = recipes\n",
    "biases_vec = biases # 重命名以避免与之前的变量混淆\n",
    "\n",
    "# 1. 计算加权和\n",
    "z = weights @ inputs\n",
    "\n",
    "# 2. 添加偏置\n",
    "z = z + biases_vec\n",
    "\n",
    "# 3. 应用激活函数 (我们用ReLU)\n",
    "activations = relu(z)\n",
    "\n",
    "print(f\"输入向量: {inputs}\")\n",
    "print(f\"权重矩阵:\\n{weights}\")\n",
    "print(f\"偏置向量: {biases_vec}\")\n",
    "print(f\"\\n步骤1+2 (加权和 + 偏置): {z}\")\n",
    "print(f\"步骤3 (最终激活值): {activations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个 `activations` 向量 `[3.6, 0.9]` 就是这一层最终的输出！它将作为下一层的输入向量，继续这个过程。\n",
    "\n",
    "## 总结与展望\n",
    "\n",
    "**恭喜！您已经完成了神经网络“静态结构”部分的学习！**\n",
    "\n",
    "我们已经完整地走过了一遍数据是如何在一个神经网络层中流动的（前向传播）：\n",
    "1.  从 **输入激活值** (向量) 开始。\n",
    "2.  通过与 **权重矩阵** 相乘，进行线性变换。\n",
    "3.  加上 **偏置向量**，调整激活阈值。\n",
    "4.  通过 **激活函数** (如ReLU) 进行非线性处理，得到 **输出激活值** (新向量)。\n",
    "\n",
    "我们现在知道了神经网络是如何进行一次“预测”的。但是，它如何“学习”呢？如何自动调整那些权重和偏置，让预测结果越来越准？\n",
    "\n",
    "这就是我们第二部分要探索的内容，我们将进入微积分的世界，学习 **损失函数**、**梯度** 和神奇的 **反向传播** 算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
