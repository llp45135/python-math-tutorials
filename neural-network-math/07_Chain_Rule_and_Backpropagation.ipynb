{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: 链式法则与反向传播 - 责任如何分配\n",
    "\n",
    "我们知道了最终的损失，也知道了梯度是降低损失的方向。但对于一个深度的、包含成千上万参数的神经网络，我们如何高效地计算出损失对每一个参数的偏导数呢？直接计算的复杂度是无法想象的。\n",
    "\n",
    "答案就是反向传播（Backpropagation），而它的数学基石，就是微积分中的 **链式法则 (Chain Rule)**。\n",
    "\n",
    "**目标:** 直观地理解链式法则，并明白反向传播是如何利用它，将最终的误差“传播”回网络中的每一个参数，从而高效计算梯度的。\n",
    "\n",
    "**核心直觉:** 追责系统。\n",
    "想象一个工厂的流水线，最终产品（神经网络的输出）出了问题（产生了损失）。为了改进，我们需要知道每个环节的工人（每个参数 `w` 和 `b`）对最终的问题负有多大的责任。\n",
    "\n",
    "反向传播就像一个高效的“追责系统”。它从最终的问题开始，一层层地往回追溯，利用链式法则，精确地计算出每个工人（参数）对最终产品问题（损失）的“贡献度”（梯度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 链式法则的直觉\n",
    "\n",
    "假设我们有一个简单的函数链： `z = f(y)` 且 `y = g(x)`。我们想知道 `x` 的微小变化如何影响 `z`，也就是求 `dz/dx`。\n",
    "\n",
    "链式法则告诉我们： `dz/dx = dz/dy * dy/dx`\n",
    "\n",
    "**直觉:** `x` 对 `z` 的总影响，等于 `y` 对 `z` 的影响，乘以 `x` 对 `y` 的影响。就像多米诺骨牌，第一个骨牌对第三个骨牌的影响，等于第二个对第三个的影响，乘以第一个对第二个的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 一个简单的计算图\n",
    "\n",
    "让我们用一个非常简单的例子来模拟神经网络的计算过程，并手动进行反向传播。\n",
    "\n",
    "假设我们有输入 `x`，参数 `w` 和 `b`，计算过程如下：\n",
    "1.  `u = w * x`\n",
    "2.  `v = u + b`\n",
    "3.  `L = v²` (我们的损失函数)\n",
    "\n",
    "我们想知道损失 `L` 对参数 `w` 和 `b` 的偏导数，即 `∂L/∂w` 和 `∂L/∂b`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向传播结果: u=6, v=10, L=100\n"
     ]
    }
   ],
   "source": [
    "# 假设一些具体值\n",
    "x = 2\n",
    "w = 3\n",
    "b = 4\n",
    "\n",
    "# --- 前向传播 ---\n",
    "u = w * x       # u = 3 * 2 = 6\n",
    "v = u + b       # v = 6 + 4 = 10\n",
    "L = v**2        # L = 10^2 = 100\n",
    "\n",
    "print(f\"前向传播结果: u={u}, v={v}, L={L}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 手动反向传播\n",
    "\n",
    "现在，我们从后往前，一步步计算偏导数。\n",
    "\n",
    "**第1步：计算 `∂L/∂v`**\n",
    "因为 `L = v²`，所以 `∂L/∂v = 2v`。\n",
    "在我们的例子中，`v=10`，所以 `∂L/∂v = 2 * 10 = 20`。\n",
    "\n",
    "**第2步：计算 `∂L/∂b` 和 `∂L/∂u`**\n",
    "我们想求 `∂L/∂b`。根据链式法则，`∂L/∂b = (∂L/∂v) * (∂v/∂b)`。\n",
    "- 我们已经知道 `∂L/∂v = 20`。\n",
    "- 因为 `v = u + b`，所以 `∂v/∂b = 1`。\n",
    "- 因此，`∂L/∂b = 20 * 1 = 20`。\n",
    "\n",
    "同理，`∂L/∂u = (∂L/∂v) * (∂v/∂u)`。\n",
    "- 因为 `v = u + b`，所以 `∂v/∂u = 1`。\n",
    "- 因此，`∂L/∂u = 20 * 1 = 20`。\n",
    "\n",
    "**第3步：计算 `∂L/∂w`**\n",
    "我们想求 `∂L/∂w`。根据链式法则，`∂L/∂w = (∂L/∂u) * (∂u/∂w)`。\n",
    "- 我们已经知道 `∂L/∂u = 20`。\n",
    "- 因为 `u = w * x`，所以 `∂u/∂w = x`。\n",
    "- 因此，`∂L/∂w = 20 * x = 20 * 2 = 40`。\n",
    "\n",
    "**结论:** 我们成功地“反向传播”了误差，得到了损失对每个参数的梯度：`∂L/∂w = 40`，`∂L/∂b = 20`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算出的梯度:\n",
      "∂L/∂w = 40\n",
      "∂L/∂b = 20\n"
     ]
    }
   ],
   "source": [
    "# --- 反向传播 ---\n",
    "\n",
    "# 1. 计算 ∂L/∂v\n",
    "dL_dv = 2 * v\n",
    "\n",
    "# 2. 计算 ∂L/∂b 和 ∂L/∂u\n",
    "dv_db = 1\n",
    "dv_du = 1\n",
    "dL_db = dL_dv * dv_db\n",
    "dL_du = dL_dv * dv_du\n",
    "\n",
    "# 3. 计算 ∂L/∂w\n",
    "du_dw = x\n",
    "dL_dw = dL_du * du_dw\n",
    "\n",
    "print(f\"计算出的梯度:\")\n",
    "print(f\"∂L/∂w = {dL_dw}\")\n",
    "print(f\"∂L/∂b = {dL_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 总结与展望\n",
    "\n",
    "**关键回顾:**\n",
    "1.  **链式法则是关键:** 它让我们能够计算嵌套函数的导数。\n",
    "2.  **反向传播是算法:** 它是一种巧妙地应用链式法则的算法，从网络的末端开始，高效地计算出每一层参数的梯度。\n",
    "3.  **先正向，再反向:** 必须先进行一次完整的前向传播来计算出每一环节的中间值，然后才能进行反向传播来计算梯度。\n",
    "\n",
    "我们已经集齐了所有必要的工具：\n",
    "- **前向传播** 来做预测。\n",
    "- **损失函数** 来衡量预测的好坏。\n",
    "- **反向传播** 来计算应该如何调整参数（梯度）。\n",
    "\n",
    "在最后一个 Notebook 中，我们将把所有这些组合在一起，实现 **梯度下降 (Gradient Descent)** 算法，真正地让我们的网络“学习”起来！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
